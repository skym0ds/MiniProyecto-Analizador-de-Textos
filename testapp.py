import nltk
from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score
from analizadortext import analizar_texto  # <-- Importa la funciÃ³n desde tu app principal


casos_prueba = [
    {
        "texto": "La Universidad Nacional AutÃ³noma de MÃ©xico es muy reconocida.",
        "palabras": ["universidad", "nacional", "autÃ³noma", "mÃ©xico"],
        "entidades": ["MÃ©xico"]
    },
    {
        "texto": "Albert Einstein desarrollÃ³ la teorÃ­a de la relatividad.",
        "palabras": ["einstein", "teorÃ­a", "relatividad"],
        "entidades": ["Albert Einstein"]
    },
    {
        "texto": "El perro corre rÃ¡pido en el parque.",
        "palabras": ["perro", "corre", "rÃ¡pido", "parque"],
        "entidades": []
    }
]

# ==========================
# EJECUCIÃ“N Y MÃ‰TRICAS
# ==========================
y_true, y_pred = [], []

for caso in casos_prueba:
    resultado = analizar_texto(caso["texto"])
    
    # EvaluaciÃ³n de palabras esperadas
    for palabra in caso["palabras"]:
        y_true.append(1)
        y_pred.append(1 if palabra in resultado.lower() else 0)

    # EvaluaciÃ³n de entidades esperadas
    for entidad in caso["entidades"]:
        y_true.append(1)
        y_pred.append(1 if entidad.lower() in resultado.lower() else 0)

    # Si no habÃ­a entidades esperadas, revisamos falsos positivos
    if not caso["entidades"]:
        y_true.append(0)
        y_pred.append(1 if "entidad" in resultado.lower() else 0)

# ==========================
# CÃLCULO DE MÃ‰TRICAS
# ==========================
accuracy = accuracy_score(y_true, y_pred)
precision = precision_score(y_true, y_pred, zero_division=0)
recall = recall_score(y_true, y_pred, zero_division=0)
f1 = f1_score(y_true, y_pred, zero_division=0)

print("ðŸ“Š RESULTADOS DE EVALUACIÃ“N ðŸ“Š")
print(f"âœ… Exactitud (Accuracy): {accuracy:.2f}")
print(f"ðŸŽ¯ PrecisiÃ³n: {precision:.2f}")
print(f"ðŸ“ˆ Recall: {recall:.2f}")
print(f"âš–ï¸ F1-Score: {f1:.2f}")
print(f"ðŸ” Total casos evaluados: {len(y_true)}")
